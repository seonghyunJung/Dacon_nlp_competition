{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init(project=\"dacon-roberta-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 27928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {\"유형\": \"type\", \"극성\": \"polarity\", \"시제\": \"tense\", \"확실성\": \"certainty\"}\n",
    "category = \"유형\"     # 유형, 극성, 시제, 확실성\n",
    "english_category = category_dict[category]   # type, polarity, tense, certainty\n",
    "pretrained_model_name_or_path = \"kykim/electra-kor-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": f\"dataset/train_data_{english_category}.csv\", \\\n",
    "                \"test\": f\"dataset/validation_data_{english_category}.csv\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "names = list(set(ds[\"train\"][category]))\n",
    "num_labels = len(names)\n",
    "cl = ClassLabel(num_classes=num_labels, names=names)\n",
    "id2label = {k: v for k, v in enumerate(cl.names)}\n",
    "\n",
    "ds = ds.cast_column(category, cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = list(set(ds[\"train\"].features) - {\"input_ids\", \"token_type_ids\", \"attention_mask\", category})\n",
    "remove_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(batch):\n",
    "    tokens = tokenizer(batch[\"문장\"], padding=\"max_length\", truncation=True)\n",
    "    return tokens\n",
    "\n",
    "ds = ds.map(tokenize_function, batched=True, remove_columns=remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.with_format(\"torch\")\n",
    "ds = ds.rename_column(category, \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfiguredMetric:\n",
    "    def __init__(self, metric, *metric_args, **metric_kwargs):\n",
    "        self.metric = metric\n",
    "        self.metric_args = metric_args\n",
    "        self.metric_kwargs = metric_kwargs\n",
    "\n",
    "    def add(self, *args, **kwargs):\n",
    "        return self.metric.add(*args, **kwargs)\n",
    "\n",
    "    def add_batch(self, *args, **kwargs):\n",
    "        return self.metric.add_batch(*args, **kwargs)\n",
    "\n",
    "    def compute(self, *args, **kwargs):\n",
    "        return self.metric.compute(*args, *self.metric_args, **kwargs, **self.metric_kwargs)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.metric.name\n",
    "\n",
    "    def _feature_names(self):\n",
    "        return self.metric._feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metrics = evaluate.combine([\n",
    "    evaluate.load('accuracy'),\n",
    "    ConfiguredMetric(evaluate.load('precision'), average='weighted'),\n",
    "    ConfiguredMetric(evaluate.load('recall'), average='weighted'),\n",
    "    ConfiguredMetric(evaluate.load('f1'), average='weighted'),\n",
    "])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metrics.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "name = f\"[{category}] \" + now.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{name}',          # output directory\n",
    "    num_train_epochs=10,             # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    gradient_accumulation_steps=8,   # Number of updates steps to accumulate the gradients for\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy= \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    do_eval=True,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    run_name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"predict\": \"data/result.csv\"}\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer = tokenizer, device=0)\n",
    "result = []\n",
    "for out in tqdm(pipe(KeyDataset(tds[\"predict\"], \"문장\"))):\n",
    "    result.append(out[\"label\"])\n",
    "\n",
    "tds[\"predict\"] = tds[\"predict\"].add_column(name=category, column=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tds[\"predict\"])\n",
    "df.to_csv(\"data/result.csv\", encoding=\"UTF-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"유형\", \"극성\", \"시제\", \"확실성\"]:\n",
    "    if label not in df.columns:\n",
    "        break\n",
    "else:\n",
    "    df[\"label\"] = df[\"유형\"] + \"-\" +  df[\"극성\"] + \"-\" + df[\"시제\"] + \"-\" +  df[\"확실성\"]\n",
    "    df = df[[\"ID\", \"label\"]]\n",
    "    df.to_csv(\"submission.csv\", encoding=\"UTF-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
